{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72752d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4322 WebAppProxyServer\n",
      "40 QuorumPeerMain\n",
      "10570 JournalNode\n",
      "2986 DFSZKFailoverController\n",
      "4236 JobHistoryServer\n",
      "20846 Jps\n",
      "10799 NameNode\n",
      "2643 JournalNode\n",
      "787 Kafka\n",
      "3733 ResourceManager\n",
      "20505 SparkSubmit\n",
      "20731 CoarseGrainedExecutorBackend\n",
      "11836 NodeManager\n",
      "11070 DataNode\n"
     ]
    }
   ],
   "source": [
    "# NameNode 확인\n",
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845136f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3337dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2fe7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import subprocess\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f860b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5a8cbb1eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917ab7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83897e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|im at ap stgate i...|    0|\n",
      "|why do i love thi...|    0|\n",
      "|nowplaying charmi...|    0|\n",
      "|im at kuwait city...|    0|\n",
      "|lolim right hande...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "801932\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def regex_(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.lower()\n",
    "    text=text[((text['total_text_list'].str.len()>= 10)) & ((text['total_text_list'].eq(' ')==False) & (text['total_text_list'].eq('')==False))]\n",
    "    return text\n",
    "def regex2_(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.lower()\n",
    "    text=text[((text['text'].str.len()>= 10)) & ((text['text'].eq(' ')==False) & (text['text'].eq('')==False))]\n",
    "    return text\n",
    "\n",
    "def regex_3(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    # @로 시작하는 애들도 정규식 이용해서 없애기 @[^ ]+\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.lower()\n",
    "    text=text[((text['full_text'].str.len()>= 10)) & ((text['full_text'].str.split(\" \").str.len()>= 10)) & ((text['full_text'].eq(' ')==False) & (text['full_text'].eq('')==False))]\n",
    "    return text\n",
    "\n",
    "def regex_4(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    # @로 시작하는 애들도 정규식 이용해서 없애기 @[^ ]+\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.lower()\n",
    "    #text=text[((text['full_text'].str.len()>= 10)) & ((text['full_text'].str.split(\" \").str.len()>= 10)) & ((text['full_text'].eq(' ')==False) & (text['full_text'].eq('')==False))]\n",
    "    return text\n",
    "\n",
    "# general_minsun2.csv\n",
    "general = pd.read_csv('/root/general.csv', sep=',',  lineterminator='\\n')\n",
    "general = general.drop_duplicates()\n",
    "general = spark.createDataFrame(general[['total_text_list']])\n",
    "general = general.select(col('total_text_list').alias('text')) \n",
    "general = general.withColumn('label', lit(0))\n",
    "general.show(5)\n",
    "print(general.count())\n",
    "general = general.withColumn(\"label\",col(\"label\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56cc2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 보안 관련 계정들 트윗\n",
    "security = pd.read_csv('/root/dataset.csv', sep=',',  lineterminator='\\n')\n",
    "security = security[[\"text\",\"datetime\"]]\n",
    "security = security.sort_values(by='datetime' ,ascending=True)\n",
    "security = security.reset_index(drop=True)\n",
    "security = regex2_(security)\n",
    "security = security[~security['text'].str.contains(\"rt\", na=False, case=False)]\n",
    "security = security.drop_duplicates()\n",
    "security = spark.createDataFrame(security[['text']])\n",
    "security = security.withColumn('label', lit(1))\n",
    "security = security.withColumn(\"label\",col(\"label\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9efaa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = security.collect()[200000:225000]\n",
    "t5 = general.collect()[200000:225000]\n",
    "\n",
    "t1 = spark.createDataFrame(t1)\n",
    "t5 = spark.createDataFrame(t5)\n",
    "\n",
    "df1 = t1.union(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62494813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                                                                           |label|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "| analysis malware exploit special easter edition c msecurity secrets daily   tinolle editioniddbcbeaaccutmcampaignpapersubutmmediumemailutmsourcesubscription  |1    |\n",
      "|trfareita  rlxb       vdf     sun  apr                                                                                                                         |1    |\n",
      "|daily analysis note upatre is back to ssl  l    malwaremustdie                                                                                                 |1    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e441f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = security.collect()[200000:200100]\n",
    "word2 = general.collect()[200000:200100]\n",
    "word1 = spark.createDataFrame(word1)\n",
    "word2= spark.createDataFrame(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568bcfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "130\n",
      "5\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def background_keyword(df): # input : spark dataframe을 pandas로 변환한 후, dataframe column\n",
    "    background = df.values.tolist()\n",
    "    token = Tokenizer()         # 토큰화 함수 지정\n",
    "    token.fit_on_texts(background)    # 토큰화 함수에 문장 적용\n",
    "    sorted_dic4 = sorted(token.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_counts_1 = {}\n",
    "    for i in sorted_dic4:\n",
    "        word_counts_1.update({i[0]:i[1]})\n",
    "    return word_counts_1 # dictionary 임\n",
    "def back_keyword(pos,neg): # pos : event-related keyword, neg : event-unrelated keyword\n",
    "    stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "    stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "    word1 = stage_1.transform(pos) #나는 first_train 부터 함!\n",
    "    word1 = stage_2.transform(word1)\n",
    "    word2 = stage_1.transform(neg) #나는 first_train 부터 함!\n",
    "    word2 = stage_2.transform(word2)\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    df1 = word1.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word1 = df1.select(\"filtered_words\").toPandas()\n",
    "    df2 = word2.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word2 = df2.select(\"filtered_words\").toPandas()\n",
    "    word_counts_1 = background_keyword(word1[\"filtered_words\"])\n",
    "    word_counts_2 = background_keyword(word2[\"filtered_words\"])\n",
    "    return word_counts_1, word_counts_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BroadcastWrapper(object):\n",
    "    def __init__(self, data, token_list):\n",
    "        self.broadcast_var = sc.broadcast(data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "        self.token_list = token_list\n",
    "    \n",
    "#     def is_should_be_updated(self, data):\n",
    "#         cur_time = datetime.now()\n",
    "#         diff_sec = (cur_time - self.last_updated_time).total_seconds()\n",
    "#         return self.broadcast_var is None or diff_sec> 1\n",
    "    \n",
    "    def update_and_get_data(self, spark):\n",
    "        a = self.broadcast_var.value\n",
    "        self.broadcast_var.unpersist()\n",
    "        for i in self.token_list:\n",
    "            for j in i:\n",
    "                if j not in a.keys():\n",
    "                    a[j] = 1\n",
    "                else:\n",
    "                    a[j] += 1\n",
    "        new_data = a\n",
    "        self.broadcast_var = spark.broadcast(new_data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "#         return len(self.token_list)\n",
    "        return self.broadcast_var\n",
    "\n",
    "# 바꾼 버전\n",
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types   \n",
    "\n",
    "def series_to_list(x):\n",
    "    seri = x.values.tolist()\n",
    "    for i in range(len(seri)):\n",
    "        seri[i] = seri[i].tolist()\n",
    "    return seri\n",
    "class TextToSequence(Transformer):\n",
    "    w1 = dict()\n",
    "    w2 = dict()\n",
    "    pos_t = Tokenizer(lower=False)\n",
    "    neg_t = Tokenizer(lower=False)\n",
    "    pos_vocab = []\n",
    "    neg_vocab = []\n",
    "    \n",
    "    def __init__(self, w1: Dict[str, int], w2:Dict[str, int]):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        result_pdf = df.select(\"filtered_words\").toPandas()\n",
    "        pos_df = result_pdf[:50000] # 24939 99756\n",
    "        neg_df = result_pdf[50000:]\n",
    "        print(len(result_pdf))\n",
    "        \n",
    "        broadcast_wrapper1 = BroadcastWrapper(self.w1, pos_df[\"filtered_words\"])\n",
    "        ww1 = broadcast_wrapper1.update_and_get_data(sc).value\n",
    "        \n",
    "        broadcast_wrapper2 = BroadcastWrapper(self.w2, neg_df[\"filtered_words\"])\n",
    "        ww2 = broadcast_wrapper2.update_and_get_data(sc).value\n",
    "        \n",
    "        www1 = {k: v for k, v in sorted(ww1.items(), key=lambda item: item[1], reverse=True)}\n",
    "        www2 = {k: v for k, v in sorted(ww2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        print(www1['allow'])\n",
    "        print(www2['like'])\n",
    "        aa = [w for i, w in enumerate(www1) if i < 5000]\n",
    "        bb = [w for i, w in enumerate(www2) if i < 5000]\n",
    "        \n",
    "        self.pos_t.fit_on_texts(aa)\n",
    "        self.neg_t.fit_on_texts(bb)\n",
    "#         encoded_docs_pos = self.pos_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "#         encoded_docs_neg = self.neg_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "        result_list = series_to_list(result_pdf[\"filtered_words\"])\n",
    "        encoded_docs_pos = self.pos_t.texts_to_sequences(result_list) # 상위 event keyword 5k가 쓰인 tokenizer\n",
    "        encoded_docs_neg = self.neg_t.texts_to_sequences(result_list)\n",
    "        \n",
    "        X_p = pad_sequences(encoded_docs_pos, maxlen=100, padding='post').tolist()\n",
    "        X_n = pad_sequences(encoded_docs_neg, maxlen=100, padding='post').tolist()\n",
    "        \n",
    "        text_array = [str(row['text']) for row in df.select('text').collect()]\n",
    "        label_array = [int(row['label']) for row in df.select('label').collect()]\n",
    "\n",
    "        \n",
    "        zip_array = list(zip(text_array, label_array, X_p, X_n))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature1','feature2'])\n",
    "        \n",
    "\n",
    "        print(type(fdf))\n",
    "\n",
    "        return fdf, www1, aa\n",
    "    \n",
    "    \n",
    "word_counts_1, word_counts_2 = back_keyword(word1, word2)\n",
    "word_counts_1 = sc.broadcast(word_counts_1)\n",
    "word_counts_2 = sc.broadcast(word_counts_2)\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "\n",
    "r1 = stage_1.transform(df1)\n",
    "r2 = stage_2.transform(r1)\n",
    "\n",
    "\n",
    "# USE THE TRANSFORMER WITHOUT PIPELINE\n",
    "text_sequence = TextToSequence(w1 = word_counts_1.value, w2 = word_counts_2.value)\n",
    "df_example, www2, bb = text_sequence.transform(r2)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"text\"], \n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature1\"]).alias(\"feature1\"), \n",
    "    list_to_vector_udf(df_example[\"feature2\"]).alias(\"feature2\")\n",
    ")\n",
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index') # transformer의 마지막 단계!!\n",
    "dataset = label_str_index.fit(df_with_vectors).transform(df_with_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b9ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataset.randomSplit(weights=[0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2567711f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|text                                                                                                     |label|feature1                                                                                                                                                                                                                                                                                                                                                                                                                          |feature2                                                                                                                                                                                                                                                                                                                                                                                                               |label_index|\n",
      "+---------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|    opensslufj rqrjtd                                                                                    |1    |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                 |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]      |1.0        |\n",
      "|  akiba pc hotline  rcooe    haxc                                                                        |1    |[852.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]               |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]      |1.0        |\n",
      "|  crypto is not pervasive easy to use the default or immune against state coercion we can use it but dont|1    |[555.0,4885.0,328.0,108.0,1489.0,356.0,108.0,11.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]|[127.0,930.0,127.0,8.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]|1.0        |\n",
      "+---------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a085e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import pickle\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D, Flatten, GlobalMaxPooling1D, Dropout, MaxPooling1D, Activation, Bidirectional, Conv2D, GlobalMaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb971d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5c02059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd\n",
    "import keras\n",
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aeb653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2436e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(2, activation='softmax', name=\"output\")(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b443a418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        {'input_type_ids': ( 0           text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'pooled_output': (N 109482241   keras_layer[0][0]                \n",
      "                                                                 keras_layer[0][1]                \n",
      "                                                                 keras_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           keras_layer_1[0][13]             \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            1538        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,483,779\n",
      "Trainable params: 1,538\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    " loss='binary_crossentropy',\n",
    " metrics=METRICS)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783cee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1109ad",
   "metadata": {},
   "source": [
    "## Option 1) Spark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b28c0ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "X_train = train.toPandas()['text']\n",
    "# X_train = np.asarray(X_train).astype(np.int)\n",
    "Y_train = train.toPandas()['label']\n",
    "# y_train=np.asarray(y_train).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69c3d293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       opensslufj rqrjtd \n",
       "1                         akiba pc hotline  rcooe    haxc \n",
       "2          crypto is not pervasive easy to use the defa...\n",
       "3         analysis malware exploit cyberwar infosec int...\n",
       "4                   dpm  dpm  bmr   mw    microsofttechnet\n",
       "                               ...                        \n",
       "40014       you must appreciate the little effort guurllol\n",
       "40015                            you picked the wrong girl\n",
       "40016                                         you too sabi\n",
       "40017    your only evidence to your claim is hes messed...\n",
       "40018                               youre obsessed with me\n",
       "Name: text, Length: 40019, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3a2a451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(Y_train, 2)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f21d058f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      " * Serving Flask app 'elephas.parameter.server' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://172.28.0.1:4000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initialize workers\n",
      ">>> Distribute load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 13:24:07] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 13:24:07] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 13:24:38] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 14:07:41] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 14:07:43] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 14:15:07] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 14:15:10] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 14:15:11] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 14:15:12] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 14:52:07] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 14:52:08] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 15:05:47] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 15:05:48] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 15:06:31] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 15:06:32] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 15:35:10] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 15:35:12] \"GET /parameters HTTP/1.1\" 200 -\n",
      "ERROR:werkzeug:45.143.201.62 - - [01/Nov/2022 15:37:25] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.143.201.62 - - [01/Nov/2022 15:37:25] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.143.201.62 - - [01/Nov/2022 15:41:32] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.143.201.62 - - [01/Nov/2022 15:41:32] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.143.201.62 - - [01/Nov/2022 15:45:56] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.143.201.62 - - [01/Nov/2022 15:45:56] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 15:55:31] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 15:55:32] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 15:56:21] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 15:56:22] \"GET /parameters HTTP/1.1\" 200 -\n",
      "ERROR:werkzeug:194.55.186.132 - - [01/Nov/2022 16:08:08] code 400, message Bad request syntax ('SSH-2.0-OpenSSH')\n",
      "INFO:werkzeug:194.55.186.132 - - [01/Nov/2022 16:08:08] \"\u001b[35m\u001b[1mSSH-2.0-OpenSSH\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 16:18:05] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 16:18:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 16:45:00] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 16:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 16:45:55] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 16:45:55] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 17:00:44] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 17:00:45] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 17:34:47] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 17:34:48] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 17:36:15] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 17:36:15] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 17:43:07] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 17:43:07] \"GET /parameters HTTP/1.1\" 200 -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 17:57:11] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 17:57:11] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:03:57] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:03:57] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:5.188.206.234 - - [01/Nov/2022 18:04:41] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:5.188.206.234 - - [01/Nov/2022 18:04:41] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:10:08] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:10:08] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:16:14] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:16:14] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:22:21] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:22:21] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 18:24:40] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 18:24:41] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 18:25:08] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 18:25:09] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 18:26:09] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 18:26:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:28:23] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:28:23] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:5.188.206.234 - - [01/Nov/2022 18:30:09] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:5.188.206.234 - - [01/Nov/2022 18:30:09] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:34:13] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:34:13] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:39:50] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:39:50] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:45:26] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:45:26] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:51:25] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:51:25] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 18:57:20] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 18:57:20] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 19:08:03] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 19:08:04] \"GET /parameters HTTP/1.1\" 200 -\n",
      "ERROR:werkzeug:45.227.254.54 - - [01/Nov/2022 19:10:46] code 400, message Bad HTTP/0.9 request type ('\\x03\\x00\\x00/*à\\x00\\x00\\x00\\x00\\x00Cookie:')\n",
      "INFO:werkzeug:45.227.254.54 - - [01/Nov/2022 19:10:47] \"\u001b[35m\u001b[1m\u0003\u0000\u0000/*à\u0000\u0000\u0000\u0000\u0000Cookie: mstshash=Administr\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 19:14:01] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 19:14:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 19:15:36] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 19:15:37] \"GET /parameters HTTP/1.1\" 200 -\n",
      "ERROR:werkzeug:23.224.186.212 - - [01/Nov/2022 19:41:20] code 400, message Bad request syntax ('\\x16\\x03\\x01\\x00¯\\x01\\x00\\x00«\\x03\\x03øsÛ<NÙðRp`¨-\\x17\\x81½\\x82®òÚ×J\\x1d\\x82^\\x82@¹~\\x85®%\\x15\\x00\\x00\\x1aÀ/À+À\\x11À\\x07À\\x13À\\tÀ\\x14À')\n",
      "INFO:werkzeug:23.224.186.212 - - [01/Nov/2022 19:41:21] \"\u001b[35m\u001b[1m\u0016\u0003\u0001\u0000¯\u0001\u0000\u0000«\u0003\u0003øsÛ<NÙðRp`¨-\u0017½®òÚ×J\u001d",
      "^@¹~",
      "®%\u0015\u0000\u0000\u001aÀ/À+À\u0011À\u0007À\u0013À\tÀ\u0014À\u001b[0m\" HTTPStatus.BAD_REQUEST -\n",
      "INFO:werkzeug:23.224.186.212 - - [01/Nov/2022 19:41:31] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:23.224.186.212 - - [01/Nov/2022 19:41:31] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:23.224.186.212 - - [01/Nov/2022 19:41:32] \"\u001b[33mGET /robots.txt HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:23.224.186.212 - - [01/Nov/2022 19:41:32] \"\u001b[33mGET /sitemap.xml HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 19:50:35] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 19:50:35] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 20:03:41] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 20:03:41] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 20:05:19] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 20:05:20] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 20:33:09] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 20:47:03] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 20:47:04] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 20:48:03] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 20:48:04] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 21:21:16] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 21:21:45] \"POST /update HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Async training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:172.28.0.1 - - [01/Nov/2022 21:21:48] \"GET /parameters HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28719.609904527664\n"
     ]
    }
   ],
   "source": [
    "from elephas.spark_model import SparkModel\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "import time\n",
    "t1 = time.time()\n",
    "rdd = to_simple_rdd(sc, X_train, y_train)\n",
    "spark_model = SparkModel(model, frequency='epoch', mode='asynchronous', num_workers=3, custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "spark_model.fit(rdd, epochs=10, batch_size=64, verbose=0, validation_split=0.1)\n",
    "t2 = time.time() - t1\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "621cae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.toPandas()['text']\n",
    "# X_train = np.asarray(X_train).astype(np.int)\n",
    "Y_test = test.toPandas()['label']\n",
    "y_test = to_categorical(Y_test, 2)\n",
    "# y_train=np.asarray(y_train).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bec1bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_model.save('bert_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d5a087c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    drweb    doctor web pacific  uemkuj \n",
       "1        analysis malware exploit cyberwar surveillanc...\n",
       "2        analysis malware exploit special easter editi...\n",
       "3        malware  hunters  aprmalware  it must die   m...\n",
       "4                                        nsa     cbmofug \n",
       "                              ...                        \n",
       "9976    whenever there is a three or four cent price a...\n",
       "9977                  who are you who are you waiting for\n",
       "9978    wish i could come join you in that bed and get...\n",
       "9979    with you gen in tyms of rain and drought  you ...\n",
       "9980    youre really beefing an interm managerwe knew ...\n",
       "Name: text, Length: 9981, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "420b5239",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 26.0 failed 4 times, most recent failure: Lost task 1.3 in stage 26.0 (TID 4765, kafka2, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3929, in _get_op_def\n    return self._op_def_cache[type]\nKeyError: 'CaseFoldUTF8'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 890, in load_internal\n    ckpt_options, filters)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 132, in __init__\n    meta_graph.graph_def.library))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 340, in load_function_def_library\n    func_graph = function_def_lib.function_def_to_graph(copy)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py\", line 59, in function_def_to_graph\n    fdef, input_shapes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py\", line 220, in function_def_to_graph_def\n    op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3934, in _get_op_def\n    buf)\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on kafka2. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/root/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/root/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\", line 228, in _evaluate\n    def _evaluate(model, optimizer, loss, custom_objects, metrics, kwargs, data_iterator):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py\", line 131, in model_from_json\n    return deserialize(config, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\", line 177, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 358, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 669, in from_config\n    config, custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1275, in reconstruct_from_config\n    process_layer(layer_data)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1257, in process_layer\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\", line 177, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 360, in deserialize_keras_object\n    return cls.from_config(cls_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 720, in from_config\n    return cls(**config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 153, in __init__\n    self._func = load_module(handle, tags, self._load_options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 449, in load_module\n    return module_v2.load(handle, tags=tags, options=set_load_options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 106, in load\n    obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 859, in load\n    return load_internal(export_dir, tags, options)[\"root\"]\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 893, in load_internal\n    str(err) + \"\\n If trying to load on a different device from the \"\nFileNotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on kafka2. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3929, in _get_op_def\n    return self._op_def_cache[type]\nKeyError: 'CaseFoldUTF8'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 890, in load_internal\n    ckpt_options, filters)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 132, in __init__\n    meta_graph.graph_def.library))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 340, in load_function_def_library\n    func_graph = function_def_lib.function_def_to_graph(copy)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py\", line 59, in function_def_to_graph\n    fdef, input_shapes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py\", line 220, in function_def_to_graph_def\n    op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3934, in _get_op_def\n    buf)\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on kafka2. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/root/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/root/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\", line 228, in _evaluate\n    def _evaluate(model, optimizer, loss, custom_objects, metrics, kwargs, data_iterator):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py\", line 131, in model_from_json\n    return deserialize(config, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\", line 177, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 358, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 669, in from_config\n    config, custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1275, in reconstruct_from_config\n    process_layer(layer_data)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1257, in process_layer\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\", line 177, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 360, in deserialize_keras_object\n    return cls.from_config(cls_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 720, in from_config\n    return cls(**config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 153, in __init__\n    self._func = load_module(handle, tags, self._load_options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 449, in load_module\n    return module_v2.load(handle, tags=tags, options=set_load_options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 106, in load\n    obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 859, in load\n    return load_internal(export_dir, tags, options)[\"root\"]\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 893, in load_internal\n    str(err) + \"\\n If trying to load on a different device from the \"\nFileNotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on kafka2. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-97bd730e5e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# predictions = spark_model.predict(X_test) # perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# perform evaluation/scoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x_test, y_test, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mtest_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_simple_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# if no metrics, we can just return the scalar corresponding to the loss value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;31m# if we do have metrics, we want to return a list of [loss value, metric value] - to match the keras API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \"\"\"\n\u001b[0;32m-> 1202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 26.0 failed 4 times, most recent failure: Lost task 1.3 in stage 26.0 (TID 4765, kafka2, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3929, in _get_op_def\n    return self._op_def_cache[type]\nKeyError: 'CaseFoldUTF8'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 890, in load_internal\n    ckpt_options, filters)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 132, in __init__\n    meta_graph.graph_def.library))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 340, in load_function_def_library\n    func_graph = function_def_lib.function_def_to_graph(copy)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py\", line 59, in function_def_to_graph\n    fdef, input_shapes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py\", line 220, in function_def_to_graph_def\n    op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3934, in _get_op_def\n    buf)\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on kafka2. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/root/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/root/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\", line 228, in _evaluate\n    def _evaluate(model, optimizer, loss, custom_objects, metrics, kwargs, data_iterator):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py\", line 131, in model_from_json\n    return deserialize(config, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\", line 177, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 358, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 669, in from_config\n    config, custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1275, in reconstruct_from_config\n    process_layer(layer_data)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1257, in process_layer\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\", line 177, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 360, in deserialize_keras_object\n    return cls.from_config(cls_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 720, in from_config\n    return cls(**config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 153, in __init__\n    self._func = load_module(handle, tags, self._load_options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 449, in load_module\n    return module_v2.load(handle, tags=tags, options=set_load_options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 106, in load\n    obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 859, in load\n    return load_internal(export_dir, tags, options)[\"root\"]\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 893, in load_internal\n    str(err) + \"\\n If trying to load on a different device from the \"\nFileNotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on kafka2. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3929, in _get_op_def\n    return self._op_def_cache[type]\nKeyError: 'CaseFoldUTF8'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 890, in load_internal\n    ckpt_options, filters)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 132, in __init__\n    meta_graph.graph_def.library))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 340, in load_function_def_library\n    func_graph = function_def_lib.function_def_to_graph(copy)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py\", line 59, in function_def_to_graph\n    fdef, input_shapes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function_def_to_graph.py\", line 220, in function_def_to_graph_def\n    op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3934, in _get_op_def\n    buf)\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on kafka2. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/root/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/root/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\", line 228, in _evaluate\n    def _evaluate(model, optimizer, loss, custom_objects, metrics, kwargs, data_iterator):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py\", line 131, in model_from_json\n    return deserialize(config, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\", line 177, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 358, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 669, in from_config\n    config, custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1275, in reconstruct_from_config\n    process_layer(layer_data)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1257, in process_layer\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\", line 177, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 360, in deserialize_keras_object\n    return cls.from_config(cls_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 720, in from_config\n    return cls(**config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 153, in __init__\n    self._func = load_module(handle, tags, self._load_options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 449, in load_module\n    return module_v2.load(handle, tags=tags, options=set_load_options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 106, in load\n    obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 859, in load\n    return load_internal(export_dir, tags, options)[\"root\"]\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 893, in load_internal\n    str(err) + \"\\n If trying to load on a different device from the \"\nFileNotFoundError: Op type not registered 'CaseFoldUTF8' in binary running on kafka2. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "# predictions = spark_model.predict(X_test) # perform inference\n",
    "evaluation = spark_model.evaluate(X_test, y_test) # perform evaluation/scoring\n",
    "t2 = time.time() - t1\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbd3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #inference 만\n",
    "\n",
    "# from tensorflow.keras.models import load_model\n",
    "# import h5py\n",
    "# import json\n",
    "\n",
    "# loaded_model = load_model('bb')\n",
    "# f = h5py.File('bb', mode='r')\n",
    "\n",
    "# elephas_conf = json.loads(f.attrs.get('distributed_config'))\n",
    "# class_name = elephas_conf.get('class_name')\n",
    "# config = elephas_conf.get('config')\n",
    "# spark_model = SparkModel(model=model, **config)\n",
    "\n",
    "# X_test = test.toPandas()['text']\n",
    "# predictions = spark_model.predict(X_test) # perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137953cb",
   "metadata": {},
   "source": [
    "## Option 2) Elephas Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42067a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_4dd63c650289"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "from elephas import spark_model, ml_model\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "from keras import optimizers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"feature2\")\n",
    "estimator.setLabelCol(\"label_index\")\n",
    "estimator.set_keras_model_config(model.to_json())\n",
    "estimator.set_categorical_labels(True) # dense 1이면 False로 설정해야함\n",
    "estimator.set_nb_classes(2)\n",
    "estimator.set_num_workers(3)\n",
    "estimator.set_custom_objects({'KerasLayer': hub.KerasLayer})\n",
    "estimator.set_epochs(10)\n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.10)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"asynchronous\")\n",
    "estimator.set_loss(\"binary_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f003387",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24c437e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "112ff7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0f61e3dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0f61e3dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0f61e3d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0f61e3d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      " * Serving Flask app 'elephas.parameter.server' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://172.28.0.1:4000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initialize workers\n",
      ">>> Distribute load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 06:26:51] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [01/Nov/2022 06:26:54] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [01/Nov/2022 06:27:00] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 06:27:45] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [01/Nov/2022 06:27:46] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [01/Nov/2022 06:27:46] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 06:28:03] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 06:28:03] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [01/Nov/2022 06:28:04] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [01/Nov/2022 06:28:20] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [01/Nov/2022 06:28:21] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [01/Nov/2022 06:28:22] \"GET /parameters HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 4 times, most recent failure: Lost task 0.3 in stage 22.0 (TID 3524, kafka3, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/worker.py\", line 109, in train\n    self.model.fit(x_train, y_train, **self.train_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py:237 call  *\n        result = smart_cond.smart_cond(training,\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py:668 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:828 __call__\n        result = self._call(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:862 _call\n        results = self._stateful_fn(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2941 __call__\n        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206 _create_graph_function\n        capture_by_value=self._capture_by_value),\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634 wrapped_fn\n        out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py:273 restored_function_body\n        \"\\n\\n\".join(signature_descriptions)))\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None, 100), dtype=string)\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/worker.py\", line 109, in train\n    self.model.fit(x_train, y_train, **self.train_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py:237 call  *\n        result = smart_cond.smart_cond(training,\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py:668 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:828 __call__\n        result = self._call(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:862 _call\n        results = self._stateful_fn(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2941 __call__\n        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206 _create_graph_function\n        capture_by_value=self._capture_by_value),\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634 wrapped_fn\n        out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py:273 restored_function_body\n        \"\\n\\n\".join(signature_descriptions)))\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None, 100), dtype=string)\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-23fbd276ab9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmy_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/ml_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    101\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                         validation_split=self.get_validation_split())\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'asynchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hogwild'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 yaml, parameters, self.client, train_config, freq, optimizer, loss, metrics, custom)\n\u001b[1;32m    179\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>> Distribute load'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>> Async training complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mnew_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 4 times, most recent failure: Lost task 0.3 in stage 22.0 (TID 3524, kafka3, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/worker.py\", line 109, in train\n    self.model.fit(x_train, y_train, **self.train_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py:237 call  *\n        result = smart_cond.smart_cond(training,\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py:668 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:828 __call__\n        result = self._call(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:862 _call\n        results = self._stateful_fn(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2941 __call__\n        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206 _create_graph_function\n        capture_by_value=self._capture_by_value),\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634 wrapped_fn\n        out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py:273 restored_function_body\n        \"\\n\\n\".join(signature_descriptions)))\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None, 100), dtype=string)\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/worker.py\", line 109, in train\n    self.model.fit(x_train, y_train, **self.train_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py:237 call  *\n        result = smart_cond.smart_cond(training,\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py:668 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:828 __call__\n        result = self._call(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:862 _call\n        results = self._stateful_fn(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2941 __call__\n        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206 _create_graph_function\n        capture_by_value=self._capture_by_value),\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634 wrapped_fn\n        out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py:273 restored_function_body\n        \"\\n\\n\".join(signature_descriptions)))\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None, 100), dtype=string)\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "my_dl = dl_pipeline.fit(train)\n",
    "t2 = time.time() - t1\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fa203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = my_dl.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d581c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")\n",
    "\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l<0.5 else 0.0 , DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    list_to_vector_udf(pred_test[\"prediction2\"]).alias(\"rawPrediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label_index')\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"rawPrediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d32b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(pred_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900f1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b689784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece37d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e62d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8cf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8cc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d314fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90627a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset=[\n",
    "    [5.0,69.0,150.0,0.0,0.0,0.0],\n",
    "    [3.0,43.0,11.0,0.0,0.0,0.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b5f5c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0f448c36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0f448c36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "X_train = train.toPandas()['feature2']\n",
    "# X_train = np.asarray(X_train).astype(np.int)\n",
    "Y_train = train.toPandas()['label']\n",
    "# y_train=np.asarray(y_train).astype(np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3d8d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train = X_train.apply(lambda x: np.array(x, dtype=np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d01d35b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79985,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79537f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79985,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47c5c85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "80105    0\n",
       "80106    0\n",
       "80107    0\n",
       "80108    0\n",
       "80109    0\n",
       "Name: label, Length: 80110, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "178dc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4eebc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = df[df['Category']=='spam']\n",
    "df_ham = df[df['Category']=='ham']\n",
    "df_ham_downsampled = df_ham.sample(df_spam.shape[0])\n",
    "df_balanced = pd.concat([df_ham_downsampled, df_spam])\n",
    "df_balanced['spam']=df_balanced['Category'].apply(lambda x: 1 if x=='spam' else 0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_balanced['Message'],df_balanced['spam'], stratify=df_balanced['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f0cb0c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607     XCLUSIVE@CLUBSAISAI 2MOROW 28/5 SOIREE SPECIAL...\n",
       "3710                 Ok.ok ok..then..whats ur todays plan\n",
       "2732    Mm feeling sleepy. today itself i shall get th...\n",
       "4569    hiya hows it going in sunny africa? hope u r a...\n",
       "3581    Have a lovely night and when you wake up to se...\n",
       "                              ...                        \n",
       "4297    Please CALL 08712402578 immediately as there i...\n",
       "3600               Then wat r u doing now? Busy wif work?\n",
       "1793    WIN: We have a winner! Mr. T. Foley won an iPo...\n",
       "1007    Panasonic & BluetoothHdset FREE. Nokia FREE. M...\n",
       "2826    Congratulations - Thanks to a good friend U ha...\n",
       "Name: Message, Length: 1120, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60ab293f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m           \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    479\u001b[0m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0;32m--> 480\u001b[0;31m                   (element, type(element).__name__))\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for 0        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n1        [1454.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....\n2        [5.0, 69.0, 150.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....\n3        [2723.0, 2583.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n4        [1334.0, 549.0, 140.0, 4839.0, 407.0, 140.0, 3...\n                               ...                        \n80105    [28.0, 580.0, 3709.0, 558.0, 606.0, 39.0, 0.0,...\n80106    [28.0, 28.0, 202.0, 1671.0, 28.0, 51.0, 34.0, ...\n80107    [28.0, 698.0, 335.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n80108    [2698.0, 787.0, 650.0, 4001.0, 191.0, 98.0, 0....\n80109    [219.0, 738.0, 155.0, 142.0, 101.0, 365.0, 1.0...\nName: feature2, Length: 80110, dtype: object with type Series",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-73cc617a8c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    379\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[1;32m    380\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     ))\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    611\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \"\"\"\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3135\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3136\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3137\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3138\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         normalized_components.append(\n\u001b[0;32m--> 111\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    112\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "model.fit(XX_train, Y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec1a9343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 915). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 915). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bb/assets\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "model.save_weights('aa')\n",
    "\n",
    "#2\n",
    "model.save('bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b34a1498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9269f08950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9269f08950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9269f08048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f9269f08048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "loaded_model = tf.keras.Model(inputs=[text_input], outputs = [l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d98a239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x7f922feb5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x7f922feb5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f922feb5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f922feb5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "weight_loaded_model = tf.keras.Model(inputs=[text_input], outputs = [l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58243639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f920eafa378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f920eafa378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f920ea99268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f920ea99268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#1\n",
    "weight_loaded_model.load_weights('aa')\n",
    "\n",
    "#2\n",
    "loaded_model = tf.keras.models.load_model('bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c0a285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = loaded_model.predict(X_test)\n",
    "y_predicted = y_predicted.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eccfa0dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.626552  , 0.71590775, 0.5898657 , 0.5472422 , 0.711022  ,\n",
       "       0.5268779 , 0.7805311 , 0.4315306 , 0.54762733, 0.71224403,\n",
       "       0.742162  , 0.6739865 , 0.6467137 , 0.7493478 , 0.5033451 ,\n",
       "       0.74692804, 0.537749  , 0.7519449 , 0.38838267, 0.73818016,\n",
       "       0.50813925, 0.51515776, 0.7246484 , 0.6430687 , 0.4697677 ,\n",
       "       0.6836265 , 0.56010693, 0.5241869 , 0.7043039 , 0.7095687 ,\n",
       "       0.66718805, 0.6527226 , 0.5277817 , 0.7622446 , 0.5987528 ,\n",
       "       0.6046336 , 0.64708745, 0.76254636, 0.86459804, 0.46739182,\n",
       "       0.54351115, 0.73788375, 0.5111824 , 0.7020923 , 0.60011864,\n",
       "       0.61944103, 0.53094864, 0.4808537 , 0.7680553 , 0.52992004,\n",
       "       0.58567214, 0.73138547, 0.65863854, 0.4457276 , 0.39486068,\n",
       "       0.52170473, 0.7092825 , 0.4048063 , 0.71466416, 0.6999085 ,\n",
       "       0.5090983 , 0.37178427, 0.79798985, 0.71043354, 0.7461667 ,\n",
       "       0.7263906 , 0.43869263, 0.50580364, 0.6773084 , 0.5050559 ,\n",
       "       0.42064878, 0.5015567 , 0.533884  , 0.7291204 , 0.7890909 ,\n",
       "       0.73485994, 0.7313714 , 0.5508839 , 0.52516437, 0.78206456,\n",
       "       0.68062484, 0.54352003, 0.6738813 , 0.7556592 , 0.7257824 ,\n",
       "       0.6494747 , 0.60281837, 0.76496875, 0.6709045 , 0.47322357,\n",
       "       0.68835354, 0.62840724, 0.62853825, 0.6948634 , 0.70504045,\n",
       "       0.66386807, 0.6369873 , 0.4962407 , 0.42247158, 0.65077287,\n",
       "       0.66103077, 0.6577848 , 0.37047455, 0.73270035, 0.5839736 ,\n",
       "       0.52414423, 0.66026247, 0.74000674, 0.44845095, 0.5888209 ,\n",
       "       0.7166858 , 0.711938  , 0.7909582 , 0.7229267 , 0.55845183,\n",
       "       0.5410729 , 0.37772185, 0.72137153, 0.7454741 , 0.6536394 ,\n",
       "       0.6008111 , 0.7592348 , 0.8260088 , 0.49554637, 0.66568017,\n",
       "       0.73436856, 0.6238657 , 0.45725647, 0.71937287, 0.6362541 ,\n",
       "       0.6458852 , 0.65134585, 0.61306345, 0.7697445 , 0.52499175,\n",
       "       0.7232676 , 0.26409176, 0.6223653 , 0.77667034, 0.7114268 ,\n",
       "       0.5373188 , 0.6654562 , 0.6549594 , 0.58316475, 0.6067015 ,\n",
       "       0.70241034, 0.62325704, 0.46031156, 0.44384012, 0.5546744 ,\n",
       "       0.5251736 , 0.54280555, 0.45638344, 0.53427804, 0.61458814,\n",
       "       0.62467444, 0.62200123, 0.4586677 , 0.64178306, 0.691227  ,\n",
       "       0.6085186 , 0.6024304 , 0.7096933 , 0.59414655, 0.7457751 ,\n",
       "       0.62553716, 0.5746772 , 0.7991743 , 0.6759219 , 0.78206456,\n",
       "       0.6815911 , 0.45885462, 0.54894185, 0.54352003, 0.51468414,\n",
       "       0.57186365, 0.65685964, 0.7632931 , 0.5139935 , 0.47748554,\n",
       "       0.67258024, 0.6977866 , 0.5337495 , 0.71436256, 0.507236  ,\n",
       "       0.4516488 , 0.49975383, 0.6304379 , 0.52408165, 0.719568  ,\n",
       "       0.6078259 , 0.7313096 , 0.71043354, 0.7391876 , 0.7320598 ,\n",
       "       0.5593362 , 0.7081133 , 0.40672225, 0.4560397 , 0.30240038,\n",
       "       0.7353209 , 0.59905   , 0.46016693, 0.4208846 , 0.47658753,\n",
       "       0.64433503, 0.6174743 , 0.7212191 , 0.7663871 , 0.7172049 ,\n",
       "       0.59223855, 0.50619596, 0.67782146, 0.6673222 , 0.63849443,\n",
       "       0.5637522 , 0.5915443 , 0.5443119 , 0.80659795, 0.63190913,\n",
       "       0.52069074, 0.44939896, 0.63255996, 0.6733373 , 0.45410863,\n",
       "       0.66307926, 0.70420736, 0.7453893 , 0.61132175, 0.77667034,\n",
       "       0.62639195, 0.73987406, 0.72952837, 0.34902102, 0.4929567 ,\n",
       "       0.7106964 , 0.51935613, 0.5877639 , 0.71662575, 0.7205725 ,\n",
       "       0.6616328 , 0.6461602 , 0.62423813, 0.66362226, 0.6977623 ,\n",
       "       0.79508877, 0.70414704, 0.7805311 , 0.5961024 , 0.713901  ,\n",
       "       0.5800992 , 0.44122016, 0.75350964, 0.7977009 , 0.7605772 ,\n",
       "       0.7185283 , 0.8020495 , 0.6652174 , 0.6372045 , 0.5140197 ,\n",
       "       0.70764136, 0.6740614 , 0.7276767 , 0.64871645, 0.6403191 ,\n",
       "       0.5876365 , 0.48072702, 0.5564844 , 0.41253993, 0.79416525,\n",
       "       0.7588846 , 0.7463729 , 0.71267545, 0.5763986 , 0.64033026,\n",
       "       0.5492361 , 0.6014207 , 0.5599791 , 0.4837083 , 0.5843145 ,\n",
       "       0.73486876, 0.7806052 , 0.52762365, 0.4486645 , 0.5927078 ,\n",
       "       0.7605772 , 0.67971694, 0.5846227 , 0.6853847 , 0.50813556,\n",
       "       0.73725736, 0.7879096 , 0.7588825 , 0.5123497 , 0.6381731 ,\n",
       "       0.39081883, 0.7592602 , 0.48401546, 0.55531275, 0.7194777 ,\n",
       "       0.7458113 , 0.52606285, 0.62207735, 0.73387617, 0.6977395 ,\n",
       "       0.45383346, 0.68134207, 0.44937357, 0.71222526, 0.5594286 ,\n",
       "       0.56117404, 0.62377745, 0.6977756 , 0.45557597, 0.40490034,\n",
       "       0.59736794, 0.6964085 , 0.31672415, 0.43354037, 0.559929  ,\n",
       "       0.6372045 , 0.6413892 , 0.6610201 , 0.44314685, 0.6381571 ,\n",
       "       0.52107024, 0.71218276, 0.7680553 , 0.64834344, 0.73517853,\n",
       "       0.58118606, 0.48599553, 0.6928022 , 0.5614104 , 0.6928449 ,\n",
       "       0.7062016 , 0.39726102, 0.72465783, 0.6414951 , 0.5454418 ,\n",
       "       0.7603799 , 0.68697256, 0.75014347, 0.544601  , 0.7250185 ,\n",
       "       0.67845243, 0.68556786, 0.7615137 , 0.516445  , 0.54390264,\n",
       "       0.51978683, 0.62671727, 0.6916536 , 0.548776  , 0.65241605,\n",
       "       0.77530336, 0.45675534, 0.48552102, 0.5397998 , 0.67909867,\n",
       "       0.5938376 , 0.5784682 , 0.6024994 , 0.7467506 , 0.5551987 ,\n",
       "       0.35874134, 0.41295245, 0.7827163 , 0.61794746, 0.52743596,\n",
       "       0.7084826 , 0.6021498 , 0.745775  , 0.6282526 ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05984cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted2 = weight_loaded_model.predict(X_test)\n",
    "y_predicted2 = y_predicted.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b97d0f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.626552  , 0.71590775, 0.5898657 , 0.5472422 , 0.711022  ,\n",
       "       0.5268779 , 0.7805311 , 0.4315306 , 0.54762733, 0.71224403,\n",
       "       0.742162  , 0.6739865 , 0.6467137 , 0.7493478 , 0.5033451 ,\n",
       "       0.74692804, 0.537749  , 0.7519449 , 0.38838267, 0.73818016,\n",
       "       0.50813925, 0.51515776, 0.7246484 , 0.6430687 , 0.4697677 ,\n",
       "       0.6836265 , 0.56010693, 0.5241869 , 0.7043039 , 0.7095687 ,\n",
       "       0.66718805, 0.6527226 , 0.5277817 , 0.7622446 , 0.5987528 ,\n",
       "       0.6046336 , 0.64708745, 0.76254636, 0.86459804, 0.46739182,\n",
       "       0.54351115, 0.73788375, 0.5111824 , 0.7020923 , 0.60011864,\n",
       "       0.61944103, 0.53094864, 0.4808537 , 0.7680553 , 0.52992004,\n",
       "       0.58567214, 0.73138547, 0.65863854, 0.4457276 , 0.39486068,\n",
       "       0.52170473, 0.7092825 , 0.4048063 , 0.71466416, 0.6999085 ,\n",
       "       0.5090983 , 0.37178427, 0.79798985, 0.71043354, 0.7461667 ,\n",
       "       0.7263906 , 0.43869263, 0.50580364, 0.6773084 , 0.5050559 ,\n",
       "       0.42064878, 0.5015567 , 0.533884  , 0.7291204 , 0.7890909 ,\n",
       "       0.73485994, 0.7313714 , 0.5508839 , 0.52516437, 0.78206456,\n",
       "       0.68062484, 0.54352003, 0.6738813 , 0.7556592 , 0.7257824 ,\n",
       "       0.6494747 , 0.60281837, 0.76496875, 0.6709045 , 0.47322357,\n",
       "       0.68835354, 0.62840724, 0.62853825, 0.6948634 , 0.70504045,\n",
       "       0.66386807, 0.6369873 , 0.4962407 , 0.42247158, 0.65077287,\n",
       "       0.66103077, 0.6577848 , 0.37047455, 0.73270035, 0.5839736 ,\n",
       "       0.52414423, 0.66026247, 0.74000674, 0.44845095, 0.5888209 ,\n",
       "       0.7166858 , 0.711938  , 0.7909582 , 0.7229267 , 0.55845183,\n",
       "       0.5410729 , 0.37772185, 0.72137153, 0.7454741 , 0.6536394 ,\n",
       "       0.6008111 , 0.7592348 , 0.8260088 , 0.49554637, 0.66568017,\n",
       "       0.73436856, 0.6238657 , 0.45725647, 0.71937287, 0.6362541 ,\n",
       "       0.6458852 , 0.65134585, 0.61306345, 0.7697445 , 0.52499175,\n",
       "       0.7232676 , 0.26409176, 0.6223653 , 0.77667034, 0.7114268 ,\n",
       "       0.5373188 , 0.6654562 , 0.6549594 , 0.58316475, 0.6067015 ,\n",
       "       0.70241034, 0.62325704, 0.46031156, 0.44384012, 0.5546744 ,\n",
       "       0.5251736 , 0.54280555, 0.45638344, 0.53427804, 0.61458814,\n",
       "       0.62467444, 0.62200123, 0.4586677 , 0.64178306, 0.691227  ,\n",
       "       0.6085186 , 0.6024304 , 0.7096933 , 0.59414655, 0.7457751 ,\n",
       "       0.62553716, 0.5746772 , 0.7991743 , 0.6759219 , 0.78206456,\n",
       "       0.6815911 , 0.45885462, 0.54894185, 0.54352003, 0.51468414,\n",
       "       0.57186365, 0.65685964, 0.7632931 , 0.5139935 , 0.47748554,\n",
       "       0.67258024, 0.6977866 , 0.5337495 , 0.71436256, 0.507236  ,\n",
       "       0.4516488 , 0.49975383, 0.6304379 , 0.52408165, 0.719568  ,\n",
       "       0.6078259 , 0.7313096 , 0.71043354, 0.7391876 , 0.7320598 ,\n",
       "       0.5593362 , 0.7081133 , 0.40672225, 0.4560397 , 0.30240038,\n",
       "       0.7353209 , 0.59905   , 0.46016693, 0.4208846 , 0.47658753,\n",
       "       0.64433503, 0.6174743 , 0.7212191 , 0.7663871 , 0.7172049 ,\n",
       "       0.59223855, 0.50619596, 0.67782146, 0.6673222 , 0.63849443,\n",
       "       0.5637522 , 0.5915443 , 0.5443119 , 0.80659795, 0.63190913,\n",
       "       0.52069074, 0.44939896, 0.63255996, 0.6733373 , 0.45410863,\n",
       "       0.66307926, 0.70420736, 0.7453893 , 0.61132175, 0.77667034,\n",
       "       0.62639195, 0.73987406, 0.72952837, 0.34902102, 0.4929567 ,\n",
       "       0.7106964 , 0.51935613, 0.5877639 , 0.71662575, 0.7205725 ,\n",
       "       0.6616328 , 0.6461602 , 0.62423813, 0.66362226, 0.6977623 ,\n",
       "       0.79508877, 0.70414704, 0.7805311 , 0.5961024 , 0.713901  ,\n",
       "       0.5800992 , 0.44122016, 0.75350964, 0.7977009 , 0.7605772 ,\n",
       "       0.7185283 , 0.8020495 , 0.6652174 , 0.6372045 , 0.5140197 ,\n",
       "       0.70764136, 0.6740614 , 0.7276767 , 0.64871645, 0.6403191 ,\n",
       "       0.5876365 , 0.48072702, 0.5564844 , 0.41253993, 0.79416525,\n",
       "       0.7588846 , 0.7463729 , 0.71267545, 0.5763986 , 0.64033026,\n",
       "       0.5492361 , 0.6014207 , 0.5599791 , 0.4837083 , 0.5843145 ,\n",
       "       0.73486876, 0.7806052 , 0.52762365, 0.4486645 , 0.5927078 ,\n",
       "       0.7605772 , 0.67971694, 0.5846227 , 0.6853847 , 0.50813556,\n",
       "       0.73725736, 0.7879096 , 0.7588825 , 0.5123497 , 0.6381731 ,\n",
       "       0.39081883, 0.7592602 , 0.48401546, 0.55531275, 0.7194777 ,\n",
       "       0.7458113 , 0.52606285, 0.62207735, 0.73387617, 0.6977395 ,\n",
       "       0.45383346, 0.68134207, 0.44937357, 0.71222526, 0.5594286 ,\n",
       "       0.56117404, 0.62377745, 0.6977756 , 0.45557597, 0.40490034,\n",
       "       0.59736794, 0.6964085 , 0.31672415, 0.43354037, 0.559929  ,\n",
       "       0.6372045 , 0.6413892 , 0.6610201 , 0.44314685, 0.6381571 ,\n",
       "       0.52107024, 0.71218276, 0.7680553 , 0.64834344, 0.73517853,\n",
       "       0.58118606, 0.48599553, 0.6928022 , 0.5614104 , 0.6928449 ,\n",
       "       0.7062016 , 0.39726102, 0.72465783, 0.6414951 , 0.5454418 ,\n",
       "       0.7603799 , 0.68697256, 0.75014347, 0.544601  , 0.7250185 ,\n",
       "       0.67845243, 0.68556786, 0.7615137 , 0.516445  , 0.54390264,\n",
       "       0.51978683, 0.62671727, 0.6916536 , 0.548776  , 0.65241605,\n",
       "       0.77530336, 0.45675534, 0.48552102, 0.5397998 , 0.67909867,\n",
       "       0.5938376 , 0.5784682 , 0.6024994 , 0.7467506 , 0.5551987 ,\n",
       "       0.35874134, 0.41295245, 0.7827163 , 0.61794746, 0.52743596,\n",
       "       0.7084826 , 0.6021498 , 0.745775  , 0.6282526 ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc8bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
